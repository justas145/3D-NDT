{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import itertools\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, Dense, Dropout, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_xy(file, crop=False):\n",
    "    images_name = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    # looading in the text file with the image name and the label\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            # get the path of an image and the label\n",
    "            image_dir, label = line.strip(\"\\n\").split(\":\")\n",
    "            if label != '-1':\n",
    "                if label =='1,2':\n",
    "                    label = 3\n",
    "                # read the image\n",
    "                image = cv2.imread(f\"{image_dir}\")\n",
    "\n",
    "                # resize to 224 x 224\n",
    "                if crop:\n",
    "                    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "                # from BGR to gray\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # append image and label to the list\n",
    "                images_name.append(image_dir)\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "        return images, labels, images_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, images_name = load_images_xy(\"XY_labels.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise and transform to np array\n",
    "def normalise_images(images, labels):\n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    labels = np.array(labels)\n",
    "    labels = labels.astype(np.int)\n",
    "\n",
    "    # Normalise the images\n",
    "    images /= 255.0\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_norm, labels = normalise_images(images, labels)\n",
    "indx = np.arange(0, len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "def shuffle_data(images_norm, labels, indx, images_name):\n",
    "    X_data, y_data, indx, images_name = sklearn.utils.shuffle(\n",
    "        images_norm, labels, indx, images_name, random_state=42)\n",
    "\n",
    "    return X_data, y_data, indx, images_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, indx, images_name = shuffle_data(\n",
    "    images_norm, labels, indx, images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping\n",
    "X_data = X_data.reshape(-1, X_data.shape[1], X_data.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder, e.g.: if it is a void then y=[0, 1]\n",
    "y_data = to_categorical(y_data, num_classes=len(np.unique(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building light CNN model\n",
    "xy_model_light = Sequential()\n",
    "xy_model_light.add(Conv2D(2, kernel_size=(3, 3), activation='linear',\n",
    "                    input_shape=(X_data.shape[1], X_data.shape[2], 1), padding='same'))\n",
    "xy_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_light.add(MaxPooling2D((2, 2), padding='same'))\n",
    "xy_model_light.add(Conv2D(4, (3, 3), activation='linear', padding='same'))\n",
    "xy_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_light.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xy_model_light.add(Conv2D(4, (3, 3), activation='linear',\n",
    "                padding='same', name='just_do_it'))\n",
    "xy_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_light.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xy_model_light.add(Flatten())\n",
    "xy_model_light.add(Dense(2, activation='linear'))\n",
    "xy_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_light.add(Dense(y_data.shape[1], activation='softmax'))\n",
    "\n",
    "xy_model_light.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                    optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving untrained model for later\n",
    "xy_model_light.save(\"xy_model_light_untrained.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building heavy CNN model\n",
    "xy_model_heavy = Sequential()\n",
    "xy_model_heavy.add(Conv2D(8, kernel_size=(3, 3), activation='linear',\n",
    "                          input_shape=(X_data.shape[1], X_data.shape[2], 1), padding='same'))\n",
    "xy_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_heavy.add(MaxPooling2D((2, 2), padding='same'))\n",
    "xy_model_heavy.add(Conv2D(16, (3, 3), activation='linear', padding='same'))\n",
    "xy_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_heavy.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xy_model_heavy.add(Conv2D(32, (3, 3), activation='linear',\n",
    "                          padding='same', name=\"just_do_it\"))\n",
    "xy_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_heavy.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xy_model_heavy.add(Flatten())\n",
    "xy_model_heavy.add(Dense(16, activation='linear'))\n",
    "xy_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xy_model_heavy.add(Dense(y_data.shape[1], activation='softmax'))\n",
    "\n",
    "xy_model_heavy.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                       optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving untrained model for later\n",
    "xy_model_heavy.save(\"xy_model_heavy_untrained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initailizing to 30 and 20 epochs, need to chech performance vs number of epochs to decide on the epochs.\n",
    "epochs_light = 30\n",
    "epochs_heavy = 20\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_heavy = xy_model_heavy.fit(X_train, y_train, epochs=epochs_heavy,\n",
    "                                   batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_light = xy_model_light.fit(X_train, y_train, epochs=epochs_light,\n",
    "                                   batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_vs_performance_plot(history):\n",
    "    # Plot the loss and accuracy curves for training and validation\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(history.history['val_loss'], color='r',\n",
    "               label=\"validation loss\", axes=ax[0])\n",
    "    ax[1].set_xlabel(\"Number of epochs\")\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'],\n",
    "               color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(history.history['val_accuracy'],\n",
    "               color='r', label=\"Validation accuracy\")\n",
    "    ax[1].set_xlabel(\"Number of epochs\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_and_acc(model, X_test, y_test):\n",
    "    # shows the accuracy of model and number of images processed per second\n",
    "    start_time = time.time()\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    delta_time = time.time() - start_time\n",
    "    num_img = X_test.shape[0]\n",
    "    print(\"--- %s images per second ---\" % (num_img/delta_time))\n",
    "    print(f\"test acc: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vs_performance_plot(history_light)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vs_performance_plot(history_heavy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciding on the epochs from the graph\n",
    "epochs_light = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciding on the epochs from the graph\n",
    "epochs_heavy = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_model_heavy = keras.models.load_model('xy_model_heavy_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_model_light = keras.models.load_model('xy_model_light_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on the new epochs number\n",
    "history_light = xy_model_light.fit(X_train, y_train, epochs=epochs_light,\n",
    "                                   batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on the new epochs number\n",
    "history_heavy = xy_model_heavy.fit(X_train, y_train, epochs=epochs_heavy,\n",
    "                                   batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking speed and accuracy\n",
    "speed_and_acc(xy_model_heavy, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking speed and accuracy\n",
    "speed_and_acc(xy_model_light, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the train set with the light model\n",
    "Y_pred = xy_model_light.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes=[\"No error\", \"Void\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the train set with the heavy model\n",
    "Y_pred = xy_model_heavy.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes=[\"No error\", \"Void\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "# returns the list of accuracies\n",
    "def cross_validation(model_name, X_data, y_data, epochs):\n",
    "    acc = []\n",
    "    kf = KFold(n_splits=5)\n",
    "    #KFold with 5 splits\n",
    "    for train_index, test_index in kf.split(X_data):\n",
    "        X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "        y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "        # reinitailizing the model\n",
    "        model = keras.models.load_model(model_name)\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                            batch_size=batch_size, validation_split=0.1)\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "        print(f\"test acc: {test_acc}\")\n",
    "        acc.append(test_acc)\n",
    "        # break because im too lazy to wait so long for cross validation, later will run it fully\n",
    "        # TODO: append test_acc to acc and make a new model with initial weights\n",
    "    return np.array(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing cross-validation on heavy model\n",
    "acc_heavy = cross_validation(\"xy_model_heavy_untrained.h5\",\n",
    "                       X_data, y_data, epochs_heavy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_acc_heavy = np.average(acc_heavy)\n",
    "st_dev_heavy = np.std(acc_heavy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average accuracy of the heavy model: {average_acc_heavy}\")\n",
    "print(f\"Standart deviation of the heavy model:{st_dev_heavy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing cross-validation on ligth model\n",
    "acc_light = cross_validation(\"xy_model_light_untrained.h5\",\n",
    "                             X_data, y_data, epochs_light)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_acc_light = np.average(acc_light)\n",
    "st_dev_light = np.std(acc_light)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average accuracy of the light model: {average_acc_light}\")\n",
    "print(f\"Standart deviation of the light model:{st_dev_light}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading untrained light model\n",
    "xy_model_light = keras.models.load_model('xy_model_light_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training light model on full dataset\n",
    "xy_model_light.fit(X_data, y_data, epochs=epochs_light,\n",
    "                                   batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_model_light.save(\"xy_model_light_FINAL.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading untrained heavy model\n",
    "xy_model_heavy = keras.models.load_model('xy_model_heavy_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training heavy model on the full dataset\n",
    "xy_model_heavy.fit(X_data, y_data, epochs=epochs_heavy,\n",
    "                   batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_model_heavy.save(\"xy_model_heavy_FINAL.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b6f61127302c8e325fb6b7b2a15bdad1f74708faa59b57333af7291771c5326"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('tf_cpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
