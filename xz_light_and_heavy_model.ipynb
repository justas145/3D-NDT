{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import Augmentor\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import itertools\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, Dense, Dropout, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of image names with plf error\n",
    "plf_image_names = []\n",
    "with open(\"xz_labels_plf.txt\") as f:\n",
    "    for line in f:\n",
    "        file, label = line.strip('\\n').split(\": \")\n",
    "        if label != \"0\":\n",
    "            plf_image_names.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy pasting images with plf to another folder for data augmentation\n",
    "src_dir = \"Processed_Images_XZ\"\n",
    "dst_dir = \"plf_XZ\"\n",
    "for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "\n",
    "    if jpgfile.replace(\"Processed_Images_XZ\\\\\", \"\") in plf_image_names:\n",
    "        shutil.copy(jpgfile, dst_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the path of the image directory (augmented images created from images with plf in this folder are deleted)\n",
    "p = Augmentor.Pipeline(\"Processed_Images_XZ\")\n",
    "# Defining augmentation parameters and generating 500 images with no error\n",
    "p.zoom(probability=0.95, min_factor=0.8, max_factor=1.2)\n",
    "p.rotate(probability=0.95, max_left_rotation=10, max_right_rotation=10)\n",
    "p.skew_corner(probability=0.8)\n",
    "p.flip_top_bottom(probability=0.95)\n",
    "p.sample(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the path of the image directory\n",
    "p = Augmentor.Pipeline(\"plf_XZ\")\n",
    "# Defining augmentation parameters and generating 1000 images with plf error\n",
    "p.zoom(probability=0.95, min_factor=0.8, max_factor=1.2)\n",
    "p.rotate(probability=0.95, max_left_rotation=10, max_right_rotation=10)\n",
    "p.skew_corner(probability=0.8)\n",
    "p.flip_top_bottom(probability=0.95)\n",
    "p.sample(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a list of augmented image names with plf error\n",
    "mypath = 'plf_XZ\\output'\n",
    "augmented_images_plf = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a list of augmented image names with no error\n",
    "mypath2 = 'Processed_Images_XZ\\output'\n",
    "augmented_images_no_error = [f for f in listdir(\n",
    "    mypath2) if isfile(join(mypath2, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that appends to the lists a label, image name and image in grayscale\n",
    "def load_images_xz(folder, image_dir, images_name, images, labels, crop=False, list=False, error=False):\n",
    "    if list == True:\n",
    "        if error == True:\n",
    "            label = '1'\n",
    "        elif error == False:\n",
    "            label = '0'\n",
    "        for file in folder:\n",
    "            # read the image Processed_Images_XZ\\output\\\n",
    "            image = cv2.imread(f\"{image_dir}{file}\")\n",
    "            # resize to 224 x 224\n",
    "            if crop:\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "\n",
    "            # from BGR to gray\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # append image and label to the list\n",
    "            images_name.append(f\"{image_dir}{file}\")\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return images, labels, images_name\n",
    "    with open(folder) as f:\n",
    "        for line in f:\n",
    "            # get the path of an image and the label\n",
    "            file, label = line.strip(\"\\n\").split(\": \")\n",
    "\n",
    "            # read the image\n",
    "            image = cv2.imread(f\"{image_dir}{file}\")\n",
    "\n",
    "            # resize to 224 x 224\n",
    "            if crop:\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "\n",
    "            # from BGR to gray\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # append image and label to the list\n",
    "            images_name.append(f\"{image_dir}{file}\")\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return images, labels, images_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name, images, labels = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, images_name = load_images_xz(\n",
    "    \"xz_labels_plf.txt\",\"Processed_Images_XZ/\", images_name, images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, images_name = load_images_xz(\n",
    "    augmented_images_plf, \"plf_XZ/output/\",  images_name, images, labels, list=True, error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, images_name = load_images_xz(\n",
    "    augmented_images_no_error, \"Processed_Images_XZ/output/\", images_name, images, labels, list=True, error=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise and transform to np array\n",
    "def normalise_images(images, labels):\n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    labels = np.array(labels)\n",
    "    labels = labels.astype(np.int)\n",
    "    # 0: no error, 1: plf error\n",
    "    labels[labels == 2] = 1\n",
    "    # Normalise the images\n",
    "    images /= 255.0\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_norm, labels = normalise_images(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the numbers are correct\n",
    "unique_labels, counts_labels = np.unique(labels, return_counts=True)\n",
    "print(np.asarray((unique_labels, counts_labels)).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "def shuffle_data(images_norm, labels, images_name):\n",
    "    X_data, y_data, images_name = sklearn.utils.shuffle(\n",
    "        images_norm, labels, images_name, random_state=42)\n",
    "\n",
    "    return X_data, y_data, images_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, images_name = shuffle_data(\n",
    "    images_norm, labels, images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping\n",
    "X_data = X_data.reshape(-1, X_data.shape[1], X_data.shape[2], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder, e.g.: if it is a plf then y=[0, 1]\n",
    "y_data = to_categorical(y_data, num_classes=len(np.unique(y_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the heavy CNN model\n",
    "xz_model_heavy = Sequential()\n",
    "xz_model_heavy.add(Conv2D(8, kernel_size=(3, 3), activation='linear',\n",
    "                          input_shape=(X_data.shape[1], X_data.shape[2], 1), padding='same'))\n",
    "xz_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_heavy.add(MaxPooling2D((2, 2), padding='same'))\n",
    "xz_model_heavy.add(Conv2D(16, (3, 3), activation='linear', padding='same'))\n",
    "xz_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_heavy.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xz_model_heavy.add(Conv2D(32, (3, 3), activation='linear',\n",
    "             padding='same', name=\"just_do_it\"))\n",
    "xz_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_heavy.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xz_model_heavy.add(Flatten())\n",
    "xz_model_heavy.add(Dense(16, activation='linear'))\n",
    "xz_model_heavy.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_heavy.add(Dense(y_data.shape[1], activation='softmax'))\n",
    "\n",
    "xz_model_heavy.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                 optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, plot=False):\n",
    "    if plot:\n",
    "        tf.keras.utils.plot_model(\n",
    "            model,\n",
    "            to_file=\"heavy_model_plot.png\",\n",
    "            show_shapes=True,\n",
    "            show_dtype=False,\n",
    "            show_layer_names=False,\n",
    "            rankdir=\"TB\",\n",
    "            expand_nested=False,\n",
    "            dpi=110,\n",
    "            layer_range=None,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving untrained heavy model for later use\n",
    "xz_model_heavy.save(\"xz_model_heavy_untrained.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating light CNN model\n",
    "xz_model_light = Sequential()\n",
    "xz_model_light.add(Conv2D(2, kernel_size=(3, 3), activation='linear',\n",
    "                          input_shape=(X_data.shape[1], X_data.shape[2], 1), padding='same'))\n",
    "xz_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_light.add(MaxPooling2D((2, 2), padding='same'))\n",
    "xz_model_light.add(Conv2D(4, (3, 3), activation='linear', padding='same'))\n",
    "xz_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_light.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xz_model_light.add(Conv2D(4, (3, 3), activation='linear',\n",
    "                          padding='same', name='just_do_it'))\n",
    "xz_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_light.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "xz_model_light.add(Flatten())\n",
    "xz_model_light.add(Dense(2, activation='linear'))\n",
    "xz_model_light.add(LeakyReLU(alpha=0.1))\n",
    "xz_model_light.add(Dense(y_data.shape[1], activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "xz_model_light.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                       optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving light model for later use\n",
    "xz_model_light.save(\"xz_model_light_untrained.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize to 20 and 30 epochs, later from learning curve decide on actual epochs\n",
    "epochs_heavy = 20\n",
    "epochs_light = 30\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70-30 train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavy model train\n",
    "history_heavy = xz_model_heavy.fit(X_train, y_train, epochs=epochs_heavy,\n",
    "                       batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light model train\n",
    "history_light = xz_model_light.fit(X_train, y_train, epochs=epochs_light,\n",
    "                                   batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_vs_performance_plot(history):\n",
    "    # Plot the loss and accuracy curves for training and validation\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(history.history['val_loss'], color='r',\n",
    "               label=\"validation loss\", axes=ax[0])\n",
    "    ax[1].set_xlabel(\"Number of epochs\")\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'],\n",
    "               color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(history.history['val_accuracy'],\n",
    "               color='r', label=\"Validation accuracy\")\n",
    "    ax[1].set_xlabel(\"Number of epochs\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_and_acc(model, X_test, y_test):\n",
    "    # shows accuracy and speed of the model\n",
    "    start_time = time.time()\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    delta_time = time.time() - start_time\n",
    "    num_img = X_test.shape[0]\n",
    "    print(\"--- %s images per second ---\" % (num_img/delta_time))\n",
    "    print(f\"test acc: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vs_performance_plot(history_light)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vs_performance_plot(history_heavy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciding on final epoch number from the plot\n",
    "epochs_light = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deciding on final epoch number from the plot\n",
    "epochs_heavy = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading untrained heavy model\n",
    "xz_model_heavy = keras.models.load_model('xz_model_heavy_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading untrained light model\n",
    "xz_model_light = keras.models.load_model('xz_model_light_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training light model with new epoch number\n",
    "history_light = xz_model_light.fit(X_train, y_train, epochs=epochs_light,\n",
    "                                   batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training heavy model with new epoch number\n",
    "history_heavy = xz_model_heavy.fit(X_train, y_train, epochs=epochs_heavy,\n",
    "                                   batch_size=batch_size, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_and_acc(xz_model_heavy, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_and_acc(xz_model_light, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the test set with the light model\n",
    "Y_pred = xz_model_light.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes=[\"No error\", \"PLF\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the test set with the heavy model\n",
    "Y_pred = xz_model_heavy.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes=[\"No error\", \"PLF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading untrained light model\n",
    "xz_model_light = keras.models.load_model('xz_model_light_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training light model on full dataset\n",
    "xz_model_light.fit(X_data, y_data, epochs=epochs_light,\n",
    "                                      batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xz_model_light.save(\"xz_model_light_FINAL.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading untrained light model\n",
    "xz_model_heavy = keras.models.load_model('xz_model_heavy_untrained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training heavy model on full dataset\n",
    "xz_model_light.fit(X_data, y_data, epochs=epochs_light,\n",
    "                   batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xz_model_heavy.save(\"xz_model_heavy_FINAL.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b6f61127302c8e325fb6b7b2a15bdad1f74708faa59b57333af7291771c5326"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('tf_cpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
